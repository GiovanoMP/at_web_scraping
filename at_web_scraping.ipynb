{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 1: \n",
    "\n",
    "Baixe seu perfil no Linkedin em PDF e utilize o PyPDF2 para construir uma função que retorne a string do texto completo do documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criaremos uma função para extrair o PDF, essa função ficará salva na pasta scripts\n",
    "\n",
    "import PyPDF2\n",
    "\n",
    "def extrair_pdf(path_pdf):\n",
    "    texto_completo = \"\"\n",
    "    \n",
    "    # Abre o arquivo PDF \n",
    "    with open(path_pdf, 'rb') as arquivo_pdf:\n",
    "        leitor_pdf = PyPDF2.PdfReader(arquivo_pdf)\n",
    "        \n",
    "        # Itera sobre todas as páginas do PDF\n",
    "        for pagina in leitor_pdf.pages:\n",
    "            texto_pagina = pagina.extract_text()\n",
    "            if texto_pagina:\n",
    "                texto_completo += texto_pagina\n",
    "    \n",
    "    return texto_completo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Chamando a função"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto salvo em: C:/infnet_ultimo_semestre/at_web_scraping/documents/texto_extraido.txt\n"
     ]
    }
   ],
   "source": [
    "# Caminho do arquivo PDF (em documents)\n",
    "caminho_pdf = r\"C:/infnet_ultimo_semestre/at_web_scraping/documents/linkedin_profile.pdf\"\n",
    "\n",
    "# Chamando a função p\n",
    "texto_do_pdf = extrair_pdf(caminho_pdf)\n",
    "\n",
    "# Caminho para salvar o arquivo de texto\n",
    "caminho_arquivo_saida = r\"C:/infnet_ultimo_semestre/at_web_scraping/documents/texto_extraido.txt\"\n",
    "\n",
    "# Salva o texto extraído em um arquivo txt\n",
    "with open(caminho_arquivo_saida, 'w', encoding='utf-8') as arquivo_saida:\n",
    "    arquivo_saida.write(texto_do_pdf)\n",
    "\n",
    "print(f\"Texto salvo em: {caminho_arquivo_saida}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 2:\n",
    "\n",
    "Utilize Regex (módulo `re` nativo do Python) para criar uma função que, a partir do texto extraído, retorne um dicionário com as seguintes informações: \n",
    "\n",
    "Seu número de telefone;\n",
    "Seu endereço de email; e \n",
    "O link do seu perfil no Linkedin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def extrair_informacoes(texto):\n",
    "    # Remover quebras de linha e espaços extras\n",
    "    texto = texto.replace('\\n', ' ').replace('  ', ' ')\n",
    "    \n",
    "    # Padrões regex para telefone, email e Linkedin\n",
    "    telefone_regex = r\"\\(?\\+?\\d{0,3}\\)?\\s?\\(?\\d{2,3}\\)?\\s?\\d{4,5}-?\\d{4}\"\n",
    "    email_regex = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "    linkedin_regex = r\"www\\.linkedin\\.com/in/[a-zA-Z0-9_-]+\"\n",
    "    \n",
    "    # Aplicando os regex\n",
    "    telefone = re.search(telefone_regex, texto)\n",
    "    email = re.search(email_regex, texto)\n",
    "    linkedin = re.search(linkedin_regex, texto)\n",
    "    \n",
    "    # Caso não houver telefone retorna \"NaN\" (meu caso)\n",
    "    telefone_resultado = telefone.group(0).strip() if telefone else np.nan\n",
    "    \n",
    "    # Captura correta do e-mail e Linkedin, removendo quebras de linha e espaços extras\n",
    "    email_resultado = email.group(0).strip() if email else np.nan\n",
    "    linkedin_resultado = linkedin.group(0).strip() if linkedin else np.nan\n",
    "    \n",
    "    # Armazenar as saídas no dicionário\n",
    "    informacoes = {\n",
    "        \"telefone\": telefone_resultado,\n",
    "        \"email\": email_resultado,\n",
    "        \"linkedin\": linkedin_resultado\n",
    "    }\n",
    "    \n",
    "    return informacoes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informações extraídas salvas\n"
     ]
    }
   ],
   "source": [
    "# Chamando a função e salvando o arquivo na mesma pasta\n",
    "\n",
    "# Lendo o texto salvo no arquivo texto_extraido.txt\n",
    "with open(r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\texto_extraido.txt', 'r', encoding='utf-8') as file:\n",
    "    texto_extraido = file.read()\n",
    "\n",
    "# Função para extrair as informações\n",
    "informacoes_extraidas = extrair_informacoes(texto_extraido)\n",
    "\n",
    "# Salvando as informações extraídas \n",
    "with open(r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\informacoes_extraidas.txt', 'w', encoding='utf-8') as file:\n",
    "    for chave, valor in informacoes_extraidas.items():\n",
    "        file.write(f'{chave}: {valor}\\n')\n",
    "\n",
    "print(\"Informações extraídas salvas\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 3:\n",
    "\n",
    "Aplique as funções geradas nas questões 1 e 2 para fazer o mesmo com o PDF em anexo (perfil do professor) e crie um CSV com todas as informações extraídas (colunas: nome, telefone, email e perfil, tanto do PDF de vocês quanto do fornecido) utilizando o módulo `csv` nativo do Python. Obs.: ao final os padrões utilizados no Regex devem abarcar os conteúdos dos dois PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** Aplicaremos as mesmas funções apenas sobre o perfil do professor Vinicius, em seguida, concatenaremos os dois arquivos .txt em um único arquivo csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parte 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo txt do salvo em: C:/infnet_ultimo_semestre/at_web_scraping/documents/texto_extraido_professor.txt\n",
      "Informações salvas.\n"
     ]
    }
   ],
   "source": [
    "# Caminho do PDF do professor (em documents)\n",
    "caminho_pdf_professor = r\"C:/infnet_ultimo_semestre/at_web_scraping/documents/ViniciusBS_Perfil.pdf\"\n",
    "\n",
    "# Extrair o texto do PDF \n",
    "texto_do_pdf_professor = extrair_pdf(caminho_pdf_professor)\n",
    "\n",
    "# Caminho para salvar o arquivo de texto \n",
    "caminho_arquivo_saida_professor = r\"C:/infnet_ultimo_semestre/at_web_scraping/documents/texto_extraido_professor.txt\"\n",
    "\n",
    "# Salva o texto extraído em um arquivo txt\n",
    "with open(caminho_arquivo_saida_professor, 'w', encoding='utf-8') as arquivo_saida:\n",
    "    arquivo_saida.write(texto_do_pdf_professor)\n",
    "\n",
    "print(f\"Arquivo txt do salvo em: {caminho_arquivo_saida_professor}\")\n",
    "\n",
    "# Lendo o texto salvo no arquivo texto_extraido_professor.txt\n",
    "with open(r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\texto_extraido_professor.txt', 'r', encoding='utf-8') as file:\n",
    "    texto_extraido_professor = file.read()\n",
    "\n",
    "# Extrair as informações do texto do professor com a segunda função\n",
    "informacoes_extraidas_professor = extrair_informacoes(texto_extraido_professor)\n",
    "\n",
    "# Salvando as informações extraídas \n",
    "with open(r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\informacoes_extraidas_professor.txt', 'w', encoding='utf-8') as file:\n",
    "    for chave, valor in informacoes_extraidas_professor.items():\n",
    "        file.write(f'{chave}: {valor}\\n')\n",
    "\n",
    "print(\"Informações salvas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informações combinadas salvas no arquivo CSV: C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\informacoes_perfis_combinadas.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Abrindo o arquivo de informações extraídas do seu perfil\n",
    "with open(r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\informacoes_extraidas.txt', 'r', encoding='utf-8') as file:\n",
    "    informacoes_perfil1 = file.read()\n",
    "\n",
    "# Abrindo o arquivo de informações extraídas do professor\n",
    "with open(r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\informacoes_extraidas_professor.txt', 'r', encoding='utf-8') as file:\n",
    "    informacoes_perfil2 = file.read()\n",
    "\n",
    "# Função auxiliar para converter as informações de string para dicionário\n",
    "def extrair_dados_do_texto(texto):\n",
    "    linhas = texto.splitlines()\n",
    "    dados = {}\n",
    "    for linha in linhas:\n",
    "        chave, valor = linha.split(\": \", 1)\n",
    "        dados[chave] = valor.strip()  # Remover possíveis espaços\n",
    "    return dados\n",
    "\n",
    "# Processando as informações extraídas\n",
    "dados_perfil1 = extrair_dados_do_texto(informacoes_perfil1)\n",
    "dados_perfil2 = extrair_dados_do_texto(informacoes_perfil2)\n",
    "\n",
    "# Nome das colunas do CSV\n",
    "colunas = ['nome', 'telefone', 'email', 'linkedin']\n",
    "\n",
    "# Dados a serem escritos no CSV\n",
    "dados = [\n",
    "    ['Giovano Panatta', dados_perfil1.get('telefone', np.nan), dados_perfil1.get('email', np.nan), dados_perfil1.get('linkedin', np.nan)],\n",
    "    ['Vinicius Branco', dados_perfil2.get('telefone', np.nan), dados_perfil2.get('email', np.nan), dados_perfil2.get('linkedin', np.nan)]\n",
    "]\n",
    "\n",
    "# Definir o caminho onde o CSV será salvo\n",
    "caminho_csv = r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\informacoes_perfis_combinadas.csv'\n",
    "\n",
    "# Salvando os dados no CSV\n",
    "with open(caminho_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(colunas)  # Escreve os cabeçalhos\n",
    "    writer.writerows(dados)   # Escreve os dados\n",
    "\n",
    "print(f\"Informações combinadas salvas no arquivo CSV: {caminho_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 4:\n",
    "\n",
    "Explore o “playground” da API do SimilarWeb encontrada no RapidAPI (https://rapidapi.com/Glavier/api/similarweb12/playground/) e inscreva-se no plano gratuito, então crie um código para obter os dados dos 10 primeiros sites listados em “top-websites”, salvando-os em um dataframe do Pandas e enfim em um arquivo CSV usando o próprio Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>favicon</th>\n",
       "      <th>rankChange</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>visitsAvgDurationFormatted</th>\n",
       "      <th>pagesPerVisit</th>\n",
       "      <th>bounceRate</th>\n",
       "      <th>isBlackListed</th>\n",
       "      <th>isNewRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google.com</td>\n",
       "      <td>https://site-images.similarcdn.com/image?url=g...</td>\n",
       "      <td>0</td>\n",
       "      <td>computers_electronics_and_technology/search_en...</td>\n",
       "      <td>00:10:45</td>\n",
       "      <td>8.146449</td>\n",
       "      <td>0.284144</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>youtube.com</td>\n",
       "      <td>https://site-images.similarcdn.com/image?url=y...</td>\n",
       "      <td>0</td>\n",
       "      <td>arts_and_entertainment/tv_movies_and_streaming</td>\n",
       "      <td>00:20:10</td>\n",
       "      <td>10.701296</td>\n",
       "      <td>0.238583</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook.com</td>\n",
       "      <td>https://site-images.similarcdn.com/image?url=f...</td>\n",
       "      <td>0</td>\n",
       "      <td>computers_electronics_and_technology/social_ne...</td>\n",
       "      <td>00:11:03</td>\n",
       "      <td>11.576611</td>\n",
       "      <td>0.304578</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>instagram.com</td>\n",
       "      <td>https://site-images.similarcdn.com/image?url=i...</td>\n",
       "      <td>0</td>\n",
       "      <td>computers_electronics_and_technology/social_ne...</td>\n",
       "      <td>00:08:34</td>\n",
       "      <td>11.612183</td>\n",
       "      <td>0.350096</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x.com</td>\n",
       "      <td>https://site-images.similarcdn.com/image?url=x...</td>\n",
       "      <td>0</td>\n",
       "      <td>computers_electronics_and_technology/social_ne...</td>\n",
       "      <td>00:11:53</td>\n",
       "      <td>12.281232</td>\n",
       "      <td>0.359107</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          domain                                            favicon  \\\n",
       "0     google.com  https://site-images.similarcdn.com/image?url=g...   \n",
       "1    youtube.com  https://site-images.similarcdn.com/image?url=y...   \n",
       "2   facebook.com  https://site-images.similarcdn.com/image?url=f...   \n",
       "3  instagram.com  https://site-images.similarcdn.com/image?url=i...   \n",
       "4          x.com  https://site-images.similarcdn.com/image?url=x...   \n",
       "\n",
       "   rankChange                                         categoryId  \\\n",
       "0           0  computers_electronics_and_technology/search_en...   \n",
       "1           0     arts_and_entertainment/tv_movies_and_streaming   \n",
       "2           0  computers_electronics_and_technology/social_ne...   \n",
       "3           0  computers_electronics_and_technology/social_ne...   \n",
       "4           0  computers_electronics_and_technology/social_ne...   \n",
       "\n",
       "  visitsAvgDurationFormatted  pagesPerVisit  bounceRate  isBlackListed  \\\n",
       "0                   00:10:45       8.146449    0.284144          False   \n",
       "1                   00:20:10      10.701296    0.238583          False   \n",
       "2                   00:11:03      11.576611    0.304578          False   \n",
       "3                   00:08:34      11.612183    0.350096          False   \n",
       "4                   00:11:53      12.281232    0.359107          False   \n",
       "\n",
       "   isNewRank  \n",
       "0      False  \n",
       "1      False  \n",
       "2      False  \n",
       "3      False  \n",
       "4      False  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando o requests\n",
    "import requests\n",
    "\n",
    "\n",
    "# url do endpoint \n",
    "url = \"https://similarweb12.p.rapidapi.com/v3/top-websites/\"\n",
    "\n",
    "# Definomdp os headers com chave da API e host\n",
    "headers = {\n",
    "    \"x-rapidapi-key\": \"f3a760c9cemsh6ba622275e00ec8p16c00fjsnfb732aa83ed1\", \n",
    "    \"x-rapidapi-host\": \"similarweb12.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "# Fazer a requisição GET\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Verificar se a requisição foi bem-sucedida\n",
    "if response.status_code == 200: # caso ok, converter para json\n",
    "    data = response.json() \n",
    "    \n",
    "    # Extrair os dados dos 10 primeiros sites usando a chave 'sites'\n",
    "    top_sites = data['sites'][:10]\n",
    "    \n",
    "    # Criar um DF com os dados dos 10 primeiros sites\n",
    "    df = pd.DataFrame(top_sites)\n",
    "    \n",
    "\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados dos 10 primeiros sites salvos no arquivo CSV: C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\top_websites.csv\n"
     ]
    }
   ],
   "source": [
    "# Definir o caminho para salvar o arquivo CSV\n",
    "caminho_csv = r'C:\\infnet_ultimo_semestre\\at_web_scraping\\documents\\top_websites.csv'\n",
    "\n",
    "# Salvar o DataFrame como um arquivo CSV\n",
    "df.to_csv(caminho_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Dados dos 10 primeiros sites salvos no arquivo CSV: {caminho_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 5:\n",
    "\n",
    "* Utilize o arquivo XML em anexo e a biblioteca `lxml` com caminhos relativos de XPath para:\n",
    "\n",
    "* Selecionar os nomes de todos *estudantes* que estejam no 2º ano ou acima dele;\n",
    "\n",
    "* Selecionar o nome do *professor* de Estruturas de Dados (course: \"Data Structures\");\n",
    "\n",
    "* Selecionar os títulos de todos os *cursos* ofertados pelo departamento de Ciência da Computação (department: Computer Science);\n",
    "\n",
    "* Selecionar os nomes de todos os *departamentos* que sejam pertencentes à Escola de Engenharia (college: Engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<university>\n",
      "    <college name=\"Engineering\">\n",
      "        <department name=\"Computer Science\" head=\"Dr. Alan Smith\">\n",
      "            <course code=\"CS101\" credits=\"4\">\n",
      "                <professor tenure=\"true\">Dr. Jane Doe</professor>\n",
      "                <title>Introduction to Computer Science</title>\n",
      "                <schedule>\n",
      "                    <day>Monday</day>\n",
      "                    <day>Wednesday</day>\n",
      "                    <time>10:00 AM - 12:00 PM</time>\n",
      "                </schedule>\n",
      "                <students>\n",
      "                    <student id=\"1001\" grade=\"A\" year=\"2\">Alice</student>\n",
      "                    <student id=\"1002\" grade=\"B+\" year=\"2\">Bob</student>\n",
      "                </students>\n",
      "            </course>\n",
      "            <course code=\"CS202\" credits=\"3\">\n",
      "                <professor tenure=\"false\">Dr. Emily Clark</professor>\n",
      "                <title>Data Structures</title>\n",
      "                <schedule>\n",
      "                    <day>Tuesday</day>\n",
      "                    <day>Thursday</day>\n",
      "                    <time>1:00 PM - 3:00 PM</time>\n",
      "                </schedule>\n",
      "                <students>\n",
      "                    <student id=\"1003\" grade=\"B-\" year=\"3\">Charlie</student>\n",
      "                    <student id=\"1004\" grade=\"A-\" year=\"3\">David</student>\n",
      "                </students>\n",
      "            </course>\n",
      "        </department>\n",
      "        <department name=\"Mechanical Engineering\" head=\"Dr. Robert Miller\">\n",
      "            <course code=\"ME101\" credits=\"4\">\n",
      "                <professor tenure=\"true\">Dr. John Lee</professor>\n",
      "                <title>Thermodynamics</title>\n",
      "                <schedule>\n",
      "                    <day>Monday</day>\n",
      "                    <day>Wednesday</day>\n",
      "                    <time>9:00 AM - 11:00 AM</time>\n",
      "                </schedule>\n",
      "                <students>\n",
      "                    <student id=\"1005\" grade=\"B\" year=\"1\">Eva</student>\n",
      "                    <student id=\"1006\" grade=\"A+\" year=\"1\">Frank</student>\n",
      "                </students>\n",
      "            </course>\n",
      "            <course code=\"ME202\" credits=\"3\">\n",
      "                <professor tenure=\"false\">Dr. William Kim</professor>\n",
      "                <title>Fluid Mechanics</title>\n",
      "                <schedule>\n",
      "                    <day>Tuesday</day>\n",
      "                    <day>Thursday</day>\n",
      "                    <time>2:00 PM - 4:00 PM</time>\n",
      "                </schedule>\n",
      "                <students>\n",
      "                    <student id=\"1007\" grade=\"C\" year=\"2\">Grace</student>\n",
      "                    <student id=\"1008\" grade=\"B+\" year=\"2\">Henry</student>\n",
      "                </students>\n",
      "            </course>\n",
      "        </department>\n",
      "    </college>\n",
      "    <college name=\"Arts\">\n",
      "        <department name=\"History\" head=\"Dr. Susan Brown\">\n",
      "            <course code=\"HIS101\" credits=\"3\">\n",
      "                <professor tenure=\"true\">Dr. Anna White</professor>\n",
      "                <title>World History</title>\n",
      "                <schedule>\n",
      "                    <day>Monday</day>\n",
      "                    <day>Wednesday</day>\n",
      "                    <time>11:00 AM - 12:30 PM</time>\n",
      "                </schedule>\n",
      "                <students>\n",
      "                    <student id=\"1009\" grade=\"A\" year=\"2\">Isla</student>\n",
      "                    <student id=\"1010\" grade=\"B\" year=\"3\">Jack</student>\n",
      "                </students>\n",
      "            </course>\n",
      "        </department>\n",
      "        <department name=\"Philosophy\" head=\"Dr. Kevin Johnson\">\n",
      "            <course code=\"PHI201\" credits=\"4\">\n",
      "                <professor tenure=\"false\">Dr. Michael Green</professor>\n",
      "                <title>Ethics</title>\n",
      "                <schedule>\n",
      "                    <day>Tuesday</day>\n",
      "                    <day>Thursday</day>\n",
      "                    <time>9:00 AM - 11:00 AM</time>\n",
      "                </schedule>\n",
      "                <students>\n",
      "                    <student id=\"1011\" grade=\"A-\" year=\"1\">Liam</student>\n",
      "                    <student id=\"1012\" grade=\"B+\" year=\"1\">Mia</student>\n",
      "                </students>\n",
      "            </course>\n",
      "        </department>\n",
      "    </college>\n",
      "</university>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carrega o arquivo XML\n",
    "xml_arquivo = 'C:/infnet_ultimo_semestre/at_web_scraping/documents/AT.xml'  \n",
    "\n",
    "# Parsear sobre XML\n",
    "tree = etree.parse(xml_arquivo)\n",
    "\n",
    "# Defini a raiz do XML para facilitar as consultas\n",
    "root = tree.getroot()\n",
    "\n",
    "# Embora possamos visualizar a estrutura no VS code, é uma boa prática visualizar via código\n",
    "print(etree.tostring(root, pretty_print=True).decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selecionar os nomes de todos *estudantes* que estejam no 2º ano ou acima dele;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'Bob', 'Charlie', 'David', 'Grace', 'Henry', 'Isla', 'Jack']\n"
     ]
    }
   ],
   "source": [
    "# Selecionando estudantes do segundo ano mais\n",
    "estudantes = root.xpath(\"//student[@year >= '2']/text()\")\n",
    "\n",
    "# Exibir os nomes \n",
    "print( estudantes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selecionar o nome do *professor* de Estruturas de Dados (course: \"Data Structures\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professor de Data Structures: ['Dr. Emily Clark']\n"
     ]
    }
   ],
   "source": [
    "# Selecionar o professor \n",
    "professor_data_structures = root.xpath(\"//course[title='Data Structures']/professor/text()\")\n",
    "\n",
    "# Exibir o nome do professor\n",
    "print(\"Professor de Data Structures:\", professor_data_structures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selecionar os títulos de todos os *cursos* ofertados pelo departamento de Ciência da Computação (department: Computer Science);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Introduction to Computer Science', 'Data Structures']\n"
     ]
    }
   ],
   "source": [
    "# títulos dos cursos do departamento de ciência da computação\n",
    "cursos_cs = root.xpath(\"//department[@name='Computer Science']/course/title/text()\")\n",
    "\n",
    "# Exibir os títulos \n",
    "print(cursos_cs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selecionar os nomes de todos os *departamentos* que sejam pertencentes à Escola de Engenharia (college: Engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Computer Science', 'Mechanical Engineering']\n"
     ]
    }
   ],
   "source": [
    "# Selecionar os nomes dos departamentos pertencentes à Escola de Engenharia\n",
    "departamentos_engineering = root.xpath(\"//college[@name='Engineering']/department/@name\")\n",
    "\n",
    "# Exibir os nomes dos departamentos\n",
    "print(departamentos_engineering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 6:\n",
    "\n",
    "Utilize o arquivo XML em anexo e a biblioteca `lxml` com seletores de CSS para:\n",
    "\n",
    "Selecionar os títulos de todos os cursos cujos professores possuem estabilidade (tenure);\n",
    "Selecionar os títulos de todos os cursos que possuem horário de início pela manhã (AM). Dica: cuidado com nomes antigos de pseudo-classes, caso algum não funcione tente o nome antigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curso com professor com estabilidade (tenure): Introduction to Computer Science\n",
      "Curso com professor com estabilidade (tenure): Thermodynamics\n",
      "Curso com professor com estabilidade (tenure): World History\n",
      "Curso matutino: Introduction to Computer Science\n",
      "Curso matutino: Thermodynamics\n",
      "Curso matutino: World History\n",
      "Curso matutino: Ethics\n"
     ]
    }
   ],
   "source": [
    "# Selecionar os títulos dos cursos com professores com estabilidade \n",
    "cursos_tenure = root.cssselect('course professor[tenure=\"true\"] ~ title')\n",
    "\n",
    "# Verificar e exibir os títulos dos cursos com professores\n",
    "if cursos_tenure:\n",
    "    for curso in cursos_tenure:\n",
    "        print(\"Curso com professor com estabilidade (tenure):\", curso.text)\n",
    "else:\n",
    "    print(\"Nenhum curso com professor com estabilidade foi encontrado\")\n",
    "\n",
    "# Buscar dentro do elemento time dentro do schedule\n",
    "cursos_am = root.cssselect('course schedule time')\n",
    "\n",
    "# Verificar se encontramos horários que contenham AM\n",
    "if cursos_am:\n",
    "    for time_element in cursos_am:\n",
    "        if \"AM\" in time_element.text:\n",
    "            title_element = time_element.xpath(\"../../title\")[0]  # Volta dois níveis para encontrar o título do curso\n",
    "            print(\"Curso matutino:\", title_element.text)\n",
    "else:\n",
    "    print(\"Nenhum curso matutino foi encontrado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 7: \n",
    "\n",
    "Examine um site de sua escolha na lista de sites fornecida em anexo e descubra o padrão de URL para paginação que ele aceita. Então, utilize-o para obter uma lista de links de notícias requisitando as 2 primeiras páginas e raspando os links de cada uma através de um único seletor de CSS aplicado via `BeautifulSoup`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para este exercício o site escolhido foi: Rondonia Dinâmica em https://www.rondoniadinamica.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/eua-anunciam-envio-de-mais-tropas-e-cacas-para-o-oriente-medio,200746.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/senador-de-rondonia-fala-sobre-seca-no-rio-madeira-e-queimadas-em-rondonia-o-homem-e-mau,200745.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/jornal-the-new-york-times-e-revista-the-new-yorker-declaram-apoio-a-kamala-harris,200744.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/haddad-ate-600-sites-de-bets-serao-banidos-do-pais-nos-proximos-dias,200743.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/contas-publicas-tem-deficit-de-r-214-bilhoes-em-agosto,200742.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/brasil-deve-registrar-nova-onda-de-calor-ate-quarta-feira,200741.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/projeto-do-campus-jaru-participa-do-4-concurso-de-qualidade-e-sustentabilidade-do-cacau-de-rondonia,200740.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/governo-realiza-acoes-de-regularizacao-fundiaria-rural-em-sao-francisco-do-guapore,200739.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/-acoes-de-educacao-ambiental-sao-reforcadas-no-festival-de-praia-de-costa-marques,200738.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/lula-defende-revisao-e-novos-acordos-comerciais-com-o-mexico,200737.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/euma-tem-48-horas-para-publicar-direito-de-resposta-de-mariana-multa-pode-chegar-a-r-15-mil-caso-de-descumprimento,200736.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/governo-intensifica-acoes-de-emergencia-nas-comunidades-indigenas-de-vilhena-e-guajara-mirim,200735.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/eleicao-no-sindicato-dos-vigilantes-de-rondonia-ocorre-dentro-da-normalidade-e-resultado-so-confirma-os-anteriores,200734.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/policia-civil-de-rondonia-deflagra-operacao-asfixia-em-cerejeiras,200733.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/regional-cafe-lanca-projeto-sintero-interativo-iniciativa-visa-facilitar-atendimentos-presenciais,200732.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/dino-marca-para-10-de-outubro-audiencia-para-debater-orcamento-secreto,200731.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/prazo-para-censo-previdenciario-2024-e-prorrogado-ate-31-de-outubro,200730.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/obras-de-revitalizacao-da-quadra-portelinha-ultrapassam-90-de-execucao-em-cujubim,200729.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/prorrogadas-inscricoes-no-premio-conciliar-e-legal-podem-ser-feitas-ate-8-de-novembro,200728.shtml\n",
      "Link encontrado: https://www.rondoniadinamica.com/noticias/2024/09/apos-debate-na-tv-expectativa-pelas-pesquisas-passagem-de-bolsonaro-influenciou-morre-pedro-andre-da-folha,200727.shtml\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Função para extrair os links de uma página\n",
    "def extrair_links(url):\n",
    "    # Fazer a requisição http com requests\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Verificar se está ok\n",
    "    if response.status_code == 200:\n",
    "        # Criar o objeto com BS\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Selecionar os links das noticias com o seletor CSS\n",
    "        links_noticias = soup.select('a.post-title[href]')\n",
    "        \n",
    "        # Adicionar a url base para transformar links relativos em completos\n",
    "        base_url = \"https://www.rondoniadinamica.com\"\n",
    "        return [base_url + link['href'] if not link['href'].startswith(base_url) else link['href'] for link in links_noticias]\n",
    "    else:\n",
    "        print(f\"Erro ao acessar a  {url}\")\n",
    "        return []\n",
    "\n",
    "# url das páginas 1 e 2 conforme o padrão de paginação\n",
    "url_pagina_1 = \"https://www.rondoniadinamica.com/ultimas-noticias?pagina=1\"\n",
    "url_pagina_2 = \"https://www.rondoniadinamica.com/ultimas-noticias?pagina=2\"\n",
    "\n",
    "# Extrair os links das páginas\n",
    "links_pagina_1 = extrair_links(url_pagina_1)\n",
    "links_pagina_2 = extrair_links(url_pagina_2)\n",
    "\n",
    "# Combinando os links\n",
    "todos_os_links = links_pagina_1 + links_pagina_2\n",
    "\n",
    "# Exibir os links \n",
    "for link in todos_os_links:\n",
    "    print(\"Link encontrado:\", link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 8:\n",
    "\n",
    "Faça um loop para os 3 primeiros links da lista obtida na questão anterior requisitando o HTML de cada página com a biblioteca que preferir (`urllib`, `requests`, etc.) e aplicando funções baseadas em `BeautifulSoup` para capturar e por fim salvar em um mesmo arquivo JSON, junto à URL de cada notícia e ao datetime do momento da requisição de cada página:\n",
    "\n",
    "* O objeto datetime (timezone-aware) da data e hora da publicação da notícia;\n",
    "* O título da notícia;\n",
    "* O corpo do texto da notícia;\n",
    "* O subtítulo da notícia (se houver);\n",
    "* O autor ou autores da notícia (se houver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informações extraídas e salvas em JSON.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Função para extrair dados da notícia\n",
    "def extrair_dados_noticia(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Capturando o título da notícia\n",
    "    titulo = soup.find('strong').text.strip() if soup.find('strong') else 'Título não encontrado'\n",
    "\n",
    "    # Capturando o autor da notícia\n",
    "    autor = soup.find('div', class_='post-excerpt').find('strong').text.strip() if soup.find('div', class_='post-excerpt') else 'Autor desconhecido'\n",
    "\n",
    "    # Capturando a data de publicação\n",
    "    data_publicacao_element = soup.find('div', class_='post-meta')\n",
    "    data_publicacao = data_publicacao_element.find_all('strong')[1].text.strip() if data_publicacao_element and len(data_publicacao_element.find_all('strong')) > 1 else 'Data não encontrada'\n",
    "    \n",
    "    # Tentativa de converter a dada\n",
    "    try:\n",
    "        data_publicacao_dt = datetime.strptime(data_publicacao, '%d/%m/%Y às %Hh%M').isoformat() if data_publicacao != 'Data não encontrada' else 'Data inválida'\n",
    "    except ValueError:\n",
    "        data_publicacao_dt = 'Data inválida'\n",
    "\n",
    "    # Capturando o corpo da notícia\n",
    "    corpo = ' '.join([p.text.strip() for p in soup.find_all('p')]) if soup.find_all('p') else 'Corpo da notícia não encontrado'\n",
    "\n",
    "    return {\n",
    "        'url': url,\n",
    "        'titulo': titulo,\n",
    "        'autor': autor,\n",
    "        'data_publicacao': data_publicacao_dt,\n",
    "        'corpo': corpo\n",
    "    }\n",
    "\n",
    "# Testando os links com a função\n",
    "links_noticias = [\n",
    "    \"https://www.rondoniadinamica.com/noticias/2024/09/pesquisa-da-futura-inteligencia-mostra-que-havera-segundo-turno-pois-mariana-teria-apenas-454-dos-votos,200659.shtml\",\n",
    "    \"https://www.rondoniadinamica.com/noticias/2024/09/ladrao-de-moto-troca-tiros-com-a-pm-apos-assalto-na-zona-leste,200658.shtml\",\n",
    "    \"https://www.rondoniadinamica.com/noticias/2024/09/jovem-e-alvejado-a-tiros-por-grupo-em-carro-residencial-porto-madero-,200657.shtml\"\n",
    "]\n",
    "\n",
    "# Processando os 3 primeiros links e salvando em um JSON\n",
    "noticias_dados = [extrair_dados_noticia(link) for link in links_noticias]\n",
    "\n",
    "# Salvando os dados no arquivo JSON\n",
    "with open('noticias_extracao_clean.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(noticias_dados, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Informações extraídas e salvas em JSON.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 10:\n",
    "\n",
    "Extraia uma lista de empregos do site https://br.indeed.com. Extraia os títulos dos empregos da primeira página de resultados ao pesquisar por \"Data Scientist\" na área da capital de seu estado. O site usa JavaScript para carregar as listas dinamicamente, o que significa que você não pode recuperar esses dados simplesmente usando solicitações ou BeautifulSoup. Escreva um script em Python usando Selenium para extrair os títulos dos empregos desta página junto a outras informações que você considere relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Título': 'Data Scientist', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=c3d2208cbc32726e&bb=eCJ5Sx3i_56t9K7q15FFZYGqyMhJsB_L6y-bhjhM9fGnMSr-GA2ZUDHDYPgoh3vJOzrcDhUfq8eUARkiHvDlriDTG-ZyqJ5lIS7YwWEUvUB8UsyEFszCq-3_9WKXCszB&xkcb=SoCJ67M37iFZxBw5Mx0LbzkdCdPP&fccid=248e71f02a698bfc&vjs=3'}, {'Título': 'Fellow Data Scientist', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=38b5d12a90da1b37&bb=eCJ5Sx3i_56t9K7q15FFZSbp9oixR9FYtHn1v700sXuzQKiCkZSnkrQaLvlS1Vmv0wrvVoQyWSNvRQwJQIigWtE0kHSrzl7e6_rXWgkDM5CQo33z4S1gEr1QbqcvBUqQ&xkcb=SoA967M37iFZxBw5Mx0KbzkdCdPP&fccid=53fac3ef8e00e16f&vjs=3'}, {'Título': 'Graduate 2024 Data Scientist, Brazil', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=37e710ba7a232648&bb=eCJ5Sx3i_56t9K7q15FFZa7FVFDiARIFcT8gSoqmsUquCxJdUTtl3PREknpXTP3lOReDo84nz6BFFpLtz5x7B5WzJI5MgbKhRbnwBlymAJpkqSQ05fJ_0cYTYA2_7QjV&xkcb=SoCg67M37iFZxBw5Mx0JbzkdCdPP&fccid=f766f8bfbc3effb7&vjs=3'}, {'Título': 'Senior Data Scientist', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=b9115aad718d12e8&bb=eCJ5Sx3i_56t9K7q15FFZUkghZ8Eaj0bltBnGyKi7Jyt74uULBtGNrELwQeQjCm2GyHhrnak5PL0qX-oNKRVvpWXO9GaxQXjOJG5t-UnfYrPRPH8y3aWZe0dr0BOF6hh&xkcb=SoAU67M37iFZxBw5Mx0IbzkdCdPP&fccid=bc90041946a5fcc9&vjs=3'}, {'Título': 'Data Scientist', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=618962f8181ff75e&bb=eCJ5Sx3i_56t9K7q15FFZR9rQLR0q9CLCbhHBGS7cJc0py-DDx7jJNwSK7Wnmx6k8iTN08s4BTDeqSljWsDHwzRACqTgwJb5Sxr2RDIXgx3D_8qlwCT0TFfa4vtZ0IMB&xkcb=SoCa67M37iFZxBw5Mx0PbzkdCdPP&fccid=0b476c4a7add47fe&vjs=3'}, {'Título': 'DATA SCIENTIST III - AVALIACAO INDEPENDENTE DE MODELOS', 'Localização': 'Osasco, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=b596397eb7eedb13&bb=eCJ5Sx3i_56t9K7q15FFZfz67Z8nudM2M2QFaUvHqNL4MTmAH2AECFJdC6aoKDQg1LJfnVmvcrZ20T4j3OHMdiL1goMmMY6RQAHhq24Gk8kscELegSgifoHY0tRBdB_V&xkcb=SoAu67M37iFZxBw5Mx0ObzkdCdPP&fccid=dd708ee3271585de&vjs=3'}, {'Título': 'Data Scientist', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=9c6fa99646f12bd6&bb=eCJ5Sx3i_56t9K7q15FFZWGE7y8EDwLmEpNhKYCT4ThRAa-QyRP_7k3EhgdU7FhH4sFvL8xAzl9EZcEYX9FJaHG-6zz_llpS6_WUcuqVbU-9zRzHWjjgAQBFp-R-DDOF&xkcb=SoCz67M37iFZxBw5Mx0NbzkdCdPP&fccid=2263de16121c2d82&vjs=3'}, {'Título': '2024 Graduate Data Scientist for Risk and Fraud, Brazil', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=ed6c697dd6d80298&bb=eCJ5Sx3i_56t9K7q15FFZbV6x4m9-534hzJKV3QK3FhbLrF0vk-k3buhzujud5CtMp-Lysg4fjj5ZP3-D8tQCkHp4tQO_wWEWBDXOVF9PeG9gy4pQuHMLZBFbwHgUvu_&xkcb=SoAH67M37iFZxBw5Mx0MbzkdCdPP&fccid=f766f8bfbc3effb7&vjs=3'}, {'Título': 'CIENTISTA JR', 'Localização': 'São José dos Campos, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=c558b1c23faaacdc&bb=eCJ5Sx3i_56t9K7q15FFZdDb88DIP-CvBN_NFtv28Q4GQu73N9UrDZX9vA6yCxfH-tFGpDE45P8LSdydRCenLP1Im9RDPQaoayYtZUUbwSjrO37E7a6cu0O6HEFXbBKt&xkcb=SoDu67M37iFZxBw5Mx0DbzkdCdPP&fccid=ed22db47960d00a3&vjs=3'}, {'Título': 'DATA SCIENTIST III', 'Localização': 'Osasco, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=7889f78735d9d15d&bb=eCJ5Sx3i_56t9K7q15FFZfdoFur8CWolggWMikVAYNwLT8kj8t-4CliQ-u1wMYqa0H6pP7RKiDa6PU27I8YNjrepRODPtYNiWgAzxJXaO6DtK_cpw_V2mHbvA1GVBBE8&xkcb=SoBa67M37iFZxBw5Mx0CbzkdCdPP&fccid=dd708ee3271585de&vjs=3'}, {'Título': 'Data Scientist Internship, Brazil – BCG X', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=6a5c18c7a1c32ca9&bb=eCJ5Sx3i_56t9K7q15FFZZUaK0_KMICS6hCcOi2xKlyTtXyLG4DuPiBaZo7DhGaVeBtAc_6z4L1e2mZGFGE2knGz5AYISiI1RljYxr0VFqV9KxXUG72SNHG9J4AVpJc2&xkcb=SoDH67M37iFZxBw5Mx0BbzkdCdPP&fccid=b95f9eb47968fa13&vjs=3'}, {'Título': 'Data Scientist', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=8e820284460324bc&bb=eCJ5Sx3i_56t9K7q15FFZYISWxGx2AR5qhHDa8GZu03cHoNHtUe_dHMCwNg92GqR1YF2Et7BKqHo_Z90jv3O_cOP-APgyurpCpbuQz6VVo6dmi-UuGhaitzR29ghrIVm&xkcb=SoBz67M37iFZxBw5Mx0AbzkdCdPP&fccid=af3399bd8ecfbafc&vjs=3'}, {'Título': 'DATA SCIENTIST III (JURÍDICO)', 'Localização': 'Osasco, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=e896b57931a8ef21&bb=eCJ5Sx3i_56t9K7q15FFZSbp9oixR9FYEl8ghFngL5kriEV6TqMnWn80VWkJegeTFri43iltsXEQV8Qmy2S-VFqe-opN5UPererDYo01s_o6vcr96vILJWErgY19pFK2&xkcb=SoD967M37iFZxBw5Mx0HbzkdCdPP&fccid=dd708ee3271585de&vjs=3'}, {'Título': 'AIOPs IT Analyst II - Data Scientist', 'Localização': 'São Paulo, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=12cf7fb94b1dde9f&bb=eCJ5Sx3i_56t9K7q15FFZXh6nSvfMWm9Qw7EnMRg7sAyNiNtar-axXQWobdnSSwHHurkDxhsCaM5an6J7up0ttIreL2GUZTWZQWnotEh81H9jbQbF4ykcA%3D%3D&xkcb=SoBJ67M37iFZxBw5Mx0GbzkdCdPP&fccid=e57a47a3d1d90e39&vjs=3'}, {'Título': 'Data scientist specialist - infrastructure & operations', 'Localização': 'Indaiatuba, SP', 'Link': 'https://br.indeed.com/rc/clk?jk=61a3d1477b63b7db&bb=eCJ5Sx3i_56t9K7q15FFZSQSokcrqNV2_OCCXlJADjUsy3oEIBw4YyEUPPjkK4eXazQDlb6J_a2bTwND5sR_SiK28Z81frrqSimnjUi3soWM7VkqsqBPjHp1b9_TrPOn&xkcb=SoDU67M37iFZxBw5Mx0FbzkdCdPP&fccid=248e71f02a698bfc&vjs=3'}]\n",
      "Informações extraídas e salvas em JSON.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Definimdo o caminho do chromedriver \n",
    "chrome_driver_path = 'C:/Program Files/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Inicializando o webdriver\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Abrir a página com os requisitos da pesquisa\n",
    "url = 'https://br.indeed.com/jobs?q=data+scientist&l=Estado+de+S%C3%A3o+Paulo'\n",
    "driver.get(url)\n",
    "\n",
    "# Tempo de espera para o carregamento 10 seg\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Aguardar que pelo menos um título de vaga \n",
    "vagas = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.slider_container')))\n",
    "\n",
    "lista_vagas = []\n",
    "\n",
    "for vaga in vagas:\n",
    "    try:\n",
    "        # Extrair o título da vaga e o link\n",
    "        titulo = vaga.find_element(By.CSS_SELECTOR, 'h2 a').text\n",
    "        link = vaga.find_element(By.CSS_SELECTOR, 'h2 a').get_attribute('href')\n",
    "    except:\n",
    "        titulo = \"Título não encontrado\"\n",
    "        link = None\n",
    "    \n",
    "    if link:\n",
    "        # Acessar a página da vaga para coletar mais informações\n",
    "        driver.get(link)\n",
    "        time.sleep(2) \n",
    "        \n",
    "        try:\n",
    "            localizacao = driver.find_element(By.CSS_SELECTOR, 'div#jobLocationText span').text\n",
    "        except:\n",
    "            localizacao = \"Localização não informada\"\n",
    "        \n",
    "        vaga_info = {\n",
    "            'Título': titulo,\n",
    "            'Localização': localizacao,\n",
    "            'Link': link\n",
    "        }\n",
    "        \n",
    "        lista_vagas.append(vaga_info)\n",
    "\n",
    "        # Voltar para a página inicial de listagem de vagas\n",
    "        driver.back()\n",
    "        time.sleep(2)\n",
    "\n",
    "# Fechar o navegador\n",
    "driver.quit()\n",
    "\n",
    "# Exibir as vagas coletadas\n",
    "print(lista_vagas)\n",
    "\n",
    "# Salvar as vagas em um arquivo JSON\n",
    "with open('vagas_data_scientist.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(lista_vagas, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Informações extraídas e salvas em JSON.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
